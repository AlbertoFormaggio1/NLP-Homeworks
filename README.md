# NLP-Homeworks
<p align="center"><img style="width:20%" src="https://upload.wikimedia.org/wikipedia/commons/thumb/4/49/Charles-University-symbol-4.png/1024px-Charles-University-symbol-4.png"></p>

Here you can find some homeworks that I did for the course Statistical Methods in Natural Language Processing I and II held by the professor Jan Hajic. <br>
I've always thought that NLP was a fascinating topic and I am glad I found a way to challenge myself by doing some work on this field.
<br><br>
Here you can find the syllabus of the aforementioned courses:
 - [Statistical Methods in Natural Language Processing I](https://ufal.mff.cuni.cz/courses/npfl067)
 - [Statistical Methods in Natural Language Processing II](https://ufal.mff.cuni.cz/courses/npfl068)
## The homeworks
 - **[Assignment 1](Text_perplexity)**: Study the perplexity and entropy of a text.
 - **[Assignment 2](EM_algorithm)**: Use the EM algorithm to get the parameters that tune the probabilities obtained from the training data over the heldout data. Finally, evaluate the model by computing the cross entropy over a separate test set.
 - **[Assignment 3](Brill's Tagger)**: Train your own Brill's tagger by defining the template and deciding appropriately the maximum number of rules
 - **[Assignment 4](HMM Tagger)**: Train an HMM Tagger both in a supervised and unsupervised way (Viterbi training and Baum-Welch training, respectively).
